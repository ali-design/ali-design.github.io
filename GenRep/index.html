<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
    <title>GenRep</title>

    <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
    <script type="text/javascript" src="jquery.mlens-1.0.min.js"></script>
    <script type="text/javascript" src="jquery.js"></script>
    <style>
        body {
            font-family: 'Open-Sans', sans-serif;
            font-weight: 300;
            background-color: #fff;
        }

        .content {
            width: 1000px;
            padding: 25px 50px;
            margin: 25px auto;
            background-color: white;
            box-shadow: 0px 0px 10px #999;
            border-radius: 15px;
        }
        .content_acc {
            width: 1000px;
            padding: 0;
            margin: 25px auto;
            background-color: white;
            box-shadow: 0px 0px 10px #999;
            border-radius: 15px;
			text-decoration: none;
        }
        .contentblock {
            width: 950px;
            margin: 0 auto;
            padding: 0;
            border-spacing: 25px 0;
        }

        .contentblock td {
            background-color: #fff;
            padding: 25px 50px;
            vertical-align: top;
            box-shadow: 0px 0px 10px #999;
            border-radius: 15px;
        }

        a,
        a:visited {
            color: #224b8d;
            font-weight: 300;
        }

        #authors {
            text-align: center;
            margin-bottom: 20px;
        }

        #conference {
            text-align: center;
            margin-bottom: 20px;
            font-style: italic;
        }

        #authors a {
            margin: 0 10px;
        }

        h1 {
            text-align: center;
            font-size: 30px;
            font-weight: 300;
        }

        h2 {
            font-size: 30px;
            font-weight: 300;
        }
        h5 {
            font-size: 10px;
            font-weight: 300;
			color:darkslategrey;
			text-align: center;
        }
        code {
            display: block;
            padding: 10px;
            margin: 10px 10px;
        }

        p {
            line-height: 25px;
            text-align: justify;
        }

        p code {
            display: inline;
            padding: 0;
            margin: 0;
        }

        #teasers {
            margin: 0 auto;
        }

        #teasers td {
            margin: 0 auto;
            text-align: center;
            padding: 5px;
        }

        #teasers img {
            width: 250px;
        }

        #results img {
            width: 133px;
        }

        #seeintodark {
            margin: 0 auto;
        }

        #sift {
            margin: 0 auto;
        }

        #sift img {
            width: 250px;
        }

        .downloadpaper {
            padding-left: 20px;
            float: right;
            text-align: center;
        }

        .downloadpaper a {
            font-weight: bold;
            text-align: center;
        }

        #demoframe {
            border: 0;
            padding: 0;
            margin: 0;
            width: 100%;
            height: 340px;
        }

        #feedbackform {
            border: 1px solid #ccc;
            margin: 0 auto;
            border-radius: 15px;
        }

        #eyeglass {
            height: 530px;
        }

        #eyeglass #wrapper {
            position: relative;
            height: auto;
            margin: 0 auto;
            float: left;
            width: 800px;
        }

        #mitnews {
            font-weight: normal;
            margin-top: 20px;
            font-size: 14px;
            width: 220px;
        }

        #mitnews a {
            font-weight: normal;
        }

        .teaser-img {
            width: 70%;
        }

        .iframe {
            width: 100%;
            height: 125%
        }
    </style>
    <!-- Global site tag (gtag.js) - Google Analytics -->
		<!--
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-98008272-2"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-98008272-2');
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
    </script>
		-->

</head>

<body>

    <div class="content">
        <h1>Generative Models as a Data Source for Multiview Representation Learning</h1>

        <p id="authors">
            <a href="http://people.csail.mit.edu/jahanian/">Ali Jahanian</a>
            <a href="https://people.csail.mit.edu/xavierpuig/">Xavier Puig</a>
            <a href="https://people.csail.mit.edu/yonglong/">Yonglong Tian</a>
            <a href="http://web.mit.edu/phillipi/">Phillip Isola</a><br>
            <!-- <strong>MIT Computer Science and Artificial Intelligence Laboratory</strong> -->
            MIT Computer Science and Artificial Intelligence Laboratory
        </p>
				<font size="+2">
					<p style="text-align: center;">
						<a href="https://openreview.net/pdf?id=qhAeZjs7dCL" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="https://github.com/ali-design/GenRep" target="_blank">[Code]</a> &nbsp;&nbsp;&nbsp;&nbsp;
<!--					    <a style="pointer-events: none; display: inline-block;" href="" target="_blank">[Video]</a> &nbsp;&nbsp;&nbsp;&nbsp;-->
						
						<a style="pointer-events: none; display: inline-block;color: gray;" href="" target="_blank">[ICLR Video: Coming Soon!]</a> &nbsp;&nbsp;&nbsp;&nbsp;
					</p>
					</font>
        <p style="text-align: center;">
            <img class="teaser-img" src='img/teaser.png'></img>
        </p>
				<!--
        <div class="downloadpaper">
            <br>
            <a href="https://arxiv.org/pdf/1907.07171.pdf"><img src="img/cover.png" width="160px" border="2">
                <p style="text-align: center;">
                    <a href="https://arxiv.org/pdf/1907.07171.pdf" target="_blank">[Paper]</a> 
                    <a href="https://github.com/ali-design/gan_steerability" target="_blank">[Code]</a>
                </p>
        </div>
				-->

        <p>Generative models are now capable of producing highly realistic images that look nearly indistinguishable from the data on which they are trained. This raises the question: if we have good enough generative models, do we still need datasets? We investigate this question in the setting of learning general-purpose visual representations from a black-box generative model rather than directly from data. Given an off-the-shelf image generator without any access to its training data, we train representations from the samples output by this generator. We compare several representation learning methods that can be applied to this setting, using the latent space of the generator to generate multiple "views" of the same semantic content. We show that for contrastive methods, this multiview data can naturally be used to identify positive pairs (nearby in latent space) and negative pairs (far apart in latent space). We find that the resulting representations rival or even outperform those learned directly from real data, but that good performance requires care in the sampling strategy applied and the training method. Generative models can be viewed as a compressed and organized copy of a dataset, and we envision a future where more and more "model zoos" proliferate while datasets become increasingly unwieldy, missing, or private. This paper suggests several techniques for dealing with visual representation learning in such a future.</p>

<!--
	        <p style="text-align: center;">
            <img class="teaser-img" src='img/teaser2.png'></img>
        </p>
-->

        <br clear="all">
    </div>
    
    <div class="content" id="Figures">

        <h2>Conceptual Figures</h2>
        <p>Different ways of creating views for contrastive learning:</p>
        <p style="text-align: center;">
            <img class="teaser-img" src='img/fig2.png'></img>
        </p>

        <p>Different ways of learning representations:</p>
        <p style="text-align: center;">
            <img class="teaser-img" src='img/fig3.png'></img>
        </p>

        <p>Different ways of creating latent transformations combined with pixel transformations:</p>
        <p style="text-align: center;">
            <img class="teaser-img" src='img/transformations.png'></img>
        </p>

    </div>      

    <div class="content" id="Press">

        <h2>Popular Press</h2>
        <p style="text-align: center;">
            <a href="https://news.mit.edu/2022/synthetic-datasets-ai-image-classification-0315" target="_blank"><img class="teaser-img" src='img/MIT-News.png'></img></a>
        </p>

    </div>   
    <div class="content" id="references">

        <h2>Reference</h2>

        <p>@article{jahanian2021generative,
  title={Generative Models as a Data Source for Multiview Representation Learning},
  author={Jahanian, Ali and Puig, Xavier and Tian, Yonglong and Isola, Phillip},
  journal={arXiv preprint arXiv:2106.05258},
  year={2021}
}</p>

        <code>
	<!--
            @article{gansteerability,<br>
            &nbsp;&nbsp;title={On the "steerability" of generative adversarial networks},<br>
            &nbsp;&nbsp;author={Jahanian, Ali and Chai, Lucy and Isola, Phillip},<br>
            &nbsp;&nbsp;journal={arXiv preprint arXiv:1907.07171},<br>
            &nbsp;&nbsp;year={2019}<br>
            }
	-->
	    
			
        </code>

    </div>      
    <div class="content" id="acknowledgements">
          <p><strong>Acknowledgements</strong>: <br>
              Author A.J. thanks Kamal Youcef-Toumi, Boris Katz, and Antonio Torralba for their support. We thank Antonio Torralba, Janne Hellsten, David Bau, and Tongzhou Wang for helpful discussions.
<p>This research was supported in part by IBM through the MIT-IBM Watson AI Lab. The research was also partly sponsored by the United States Air Force Research Laboratory and the United States Air Force Artificial Intelligence Accelerator and was accomplished under Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the United States Air Force or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein. 
		</p>
 
<!--              Website template is borrowed from our <a href="http://ganalyze.csail.mit.edu/">memorable friends</a>.-->
              <!--
              <p><strong>Disclaimer</strong>: The views and conclusions contained herein are those of the authors and
                  should not be interpreted as necessarily representing the official policies or endorsements, either
                  expressed or implied, of IARPA, DOI/IBC, or the U.S.
              </p>
              -->
    </div>
<div class="content_acc">
<a href="https://accessibility.mit.edu/"><h5>MIT Accessibility</h5></a>
</div>
</body>

</html>
